# Lecture 6: Monte Carlo Simulation

Tags: #algorithms #python #modeling #simulation #montecarlo

## Introduction

- Invented by Stanislaw Ulam (a polish mathematician). Was ill and playing a lot of solitarie.
- Failed to compute the combinatorics of winning.
- Asked Von Neumann to simulate it.
- Estimating the value of an unknown quantity using the principles of inferential statistics.
- **Population:** set of examples.
- **Sample:** a proper subset of a population.
- **Random sample** tends to exhibit the same properties as the population from which it is drawn.
- Exactly what we did with random walk.

## Example: Flipping a Coin

Confidence in the estimate depends upon:

- Size of the sample.
- Variance of sample.

## Roulette

```python
class FairRoulette():
    def __init__(self):
        self.pockets = []
        for i in range(1,37):
            self.pockets.append(i)
        self.ball = None
        self.pocketOdds = len(self.pockets) - 1
    def spin(self):
        self.ball = random.choice(self.pockets)
    def betPocket(self, pocket, amt):
        if str(pocket) == str(self.ball):
            return amt*self.pocketOdds
        else: return -amt
    def __str__(self):
        return 'Fair Roulette'
      
def playRoulette(game, numSpins, pocket, bet, toPrint):
    totPocket = 0
    for i in range(numSpins):
        game.spin()
        totPocket += game.betPocket(pocket, bet)
    if toPrint:
        print(numSpins, 'spins of', game)
        print('Expected return betting', pocket, '=',\
              str(100*totPocket/numSpins) + '%\n')
    return (totPocket/numSpins)

random.seed(0)
game = FairRoulette()
for numSpins in (100, 1000000):
    for i in range(3):
        playRoulette(game, numSpins, 2, 1, True)  
```

## Law of Large Numbers

> In repeated independent tests with the same actual probability p of a particular outcome in each test, the chance that the fraction of times that outcome occurs differs from p converges to zero as the number of trials goes to infinity 

### Gambler's Fallacy

> Does this imply that if deviations from expected behavior occur, these deviations are likely to be evened out by opposite deviations in the future?

### Regression to the Mean

After an extreme random event, the next random event is likely to be less extreme.

## Unfair Casinos

```python
class EuRoulette(FairRoulette):
    def __init__(self):
        FairRoulette.__init__(self)
        self.pockets.append('0')
    def __str__(self):
        return 'European Roulette'

class AmRoulette(EuRoulette):
    def __init__(self):
        EuRoulette.__init__(self)
        self.pockets.append('00')
    def __str__(self):
        return 'American Roulette'
        
def findPocketReturn(game, numTrials, trialSize, toPrint):
    pocketReturns = []
    for t in range(numTrials):
        trialVals = playRoulette(game, trialSize, 2, 1, toPrint)
        pocketReturns.append(trialVals)
    return pocketReturns

random.seed(0)
numTrials = 20
resultDict = {}
games = (FairRoulette, EuRoulette, AmRoulette)
for G in games:
    resultDict[G().__str__()] = []
for numSpins in (1000, 10000, 100000, 1000000):
    print('\nSimulate', numTrials, 'trials of',
          numSpins, 'spins each')
    for G in games:
        pocketReturns = findPocketReturn(G(), numTrials,
                                         numSpins, False)
        expReturn = 100*sum(pocketReturns)/len(pocketReturns)
        print('Exp. return for', G(), '=',
             str(round(expReturn, 4)) + '%')
```

How many samples do we need to have justifiable confidence in our answer?

It depends upon variability in underlying distribution.
$$
\text{variance}(x) = \frac{\sum_{x\in X}(x-\mu)^2}{|X|}
$$

$$
\sigma(X)=\sqrt{\frac{1}{|X|}\sum_{x\in X}(x-\mu)^2}
$$

Where $\mu$ is the mean and $X$ is the size of the sample.

- Standard deviation is simply the square root of the variance.
- Outliers have a big effect.
- Standard deviation should always be considered relative to mean.

```python
def getMeanAndStd(X):
    mean = sum(X)/float(len(X))
    tot = 0.0
    for x in X:
        tot += (x - mean)**2
    std = (tot/len(X))**0.5
    return mean, std
```

## Confidence Levels and Intervals

Provides a range that is likely to contain the unknown value and a confidence that the unknown value lays within that range.

How to compute them?

**Empirical rule:**

- 68% within 1 STD
- 95% within 1.96 STD (most used)
- 99.7% within 3 STD

```python
random.seed(0)
numTrials = 20
resultDict = {}
games = (FairRoulette, EuRoulette, AmRoulette)
for G in games:
    resultDict[G().__str__()] = []
for numSpins in (1000, 10000, 100000, 1000000):
    print('\nSimulate betting a pocket for', numTrials, 'trials of',
          numSpins, 'spins each')
    for G in games:
        pocketReturns = findPocketReturn(G(), numTrials,
                                         numSpins, False)
        mean, std = getMeanAndStd(pocketReturns)
        resultDict[G().__str__()].append((numSpins, 100*mean, 100*std))
        print('Exp. return for', G(), '=',
              str(round(100*mean, 3))
              + '%,', '+/- '
              + str(round(100*1.96*std, 3))
              + '% with 95% confidence')
```

## Defining Distributions

Captures notion of relative frequency with which a random variable takes on certain values.

For discrete variable (drawn from finite set of values), simply list the probability of each value, must add up to 1.

Continuous case (infinite set of values) trickier, can't enumerate probability for each of an infinite set of values.

Probability Density Functions - PDFs.

Probability of a random variable lies between two numbers.

Given a curve where the values on the x-axis lie between the min and max value of the variable, the area under the curve between two points is the probability of example falling within the range.

### Normal Distributions

$$
P(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

$$
e = \sum_{n=0}^{\infty}\frac{1}{n!}
$$

Why does this work?