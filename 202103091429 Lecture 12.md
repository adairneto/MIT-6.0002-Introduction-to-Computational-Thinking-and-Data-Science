# Lecture 12: Clustering

Tags: #algorithms #python #machinelearning

## Clustering is an Optimization Problem

$$
\text{variability}(c) = \sum_{e\in c}\text{distance}(\text{mean}(c),e)^2
$$

$$
\text{dissimilarity}(C) = \sum_{c\in C}\text{variability}(c)
$$

If we divide the variability by the size of the cluster, we get the variance. It would normalize, but big and bad is worse than small and bad.

The problem isn't to find a C that minimizes dissimilarity because that would put each example in its own cluster.

Hence, we need a constraint, which can be:

- Minimum distance between clusters
- Number of clusters

Two methods:

## Hierarchical clustering

1. Assign each item to a cluster
2. Find the closest pair of clusters and merge them into a single cluster
3. Continue until we get a single cluster of size n (the idea is to stop somewhere along the way: **dendogram**)

### What do mean by distance?

**Linkage Metrics:**

- Single-linkage: the distance between the clusters is equal to the shortest distance from any member of one cluster to any member of the other cluster
- Complete-linkage: the distance between the clusters is equal to the greatest distance from any member to any member
- Average-linkage: the distance between the clusters is equal to the average distance between any member to any member

It's a deterministic algorithm.

It's a greedy/naïve algorithm. $n^3$ complexity.

## K-means clustering

Is a much faster greedy algorithm.

Most useful when you know how many clusters you want.

```pseudocode
randomly chose k examples as initial centroids
while true:
	create k clusters by assigning each example to closest centroid
	compute k new centroids by averaging examples in each cluster
	if centroids don’t change:
		break
```

What is complexity of one iteration? $k\cdot n \cdot d$ where $d$ is the time to compute the distance.

K-means is not deterministic.

Results depend upon initial centroids, number of iterations.

### How to choose K?

- A priori knowledge about application domain
- Search for a good k (better approach)
  - Try different values of k and evaluate the quality of result
  - Run hierarchical clustering on subset of data

### Mitigate dependence on initial centroids

- Try multiple sets of randomly chosen initial centroids
- Select best result

```python
best = kMeans(points)
for t in range(numTrials):
	C = kMeans(points)
	if dissimilarity(C) < dissimilarity(best):
		best = C
return best
```

## An Example

- Many patients with 4 features each:
  - Heart rate in beats per minute
  - Number of past heart attacks
  - Age
  - ST elevation (binary)
- Outcome (death) based on features
  - Probabilistic, not deterministic
  - E.g. older people with multiple heart attacks at higher risk
- Cluster, and examine purity of clusters relative to outcomes

Base code of cluster.py:

```python
import pylab

#set line width
pylab.rcParams['lines.linewidth'] = 6
#set general font size 
pylab.rcParams['font.size'] = 12
#set font size for labels on axes
pylab.rcParams['axes.labelsize'] = 18
#set size of numbers on x-axis
pylab.rcParams['xtick.major.size'] = 5
#set size of numbers on y-axis
pylab.rcParams['ytick.major.size'] = 5
#set size of markers
pylab.rcParams['lines.markersize'] = 10

def minkowskiDist(v1, v2, p):
    #Assumes v1 and v2 are equal length arrays of numbers
    dist = 0
    for i in range(len(v1)):
        dist += abs(v1[i] - v2[i])**p
    return dist**(1/p)
```

Creating Class Example:

```python
class Example(object):
    
    def __init__(self, name, features, label = None):
        #Assumes features is an array of floats
        self.name = name
        self.features = features
        self.label = label
        
    def dimensionality(self):
        return len(self.features)
    
    def getFeatures(self):
        return self.features[:]
    
    def getLabel(self):
        return self.label
    
    def getName(self):
        return self.name
    
    def distance(self, other):
        return minkowskiDist(self.features, other.getFeatures(), 2) # Euclidian distance
    
    def __str__(self):
        return self.name +':'+ str(self.features) + ':'\
               + str(self.label)
```

Class Cluster:

```python
class Cluster(object):
    
    def __init__(self, examples):
        """Assumes examples a non-empty list of Examples"""
        self.examples = examples
        self.centroid = self.computeCentroid()
        
    def update(self, examples):
        """Assume examples is a non-empty list of Examples
           Replace examples; return amount centroid has changed"""
        oldCentroid = self.centroid
        self.examples = examples
        self.centroid = self.computeCentroid()
        return oldCentroid.distance(self.centroid)
    
    def computeCentroid(self):
        vals = pylab.array([0]*self.examples[0].dimensionality())
        for e in self.examples: #compute mean
            vals += e.getFeatures()
        centroid = Example('centroid', vals/len(self.examples))
        return centroid

    def getCentroid(self):
        return self.centroid

    def variability(self):
        totDist = 0
        for e in self.examples:
            totDist += (e.distance(self.centroid))**2
        return totDist
        
    def members(self): # Makes possible to iterate through examples
        for e in self.examples:
            yield e

    def __str__(self):
        names = []
        for e in self.examples:
            names.append(e.getName())
        names.sort()
        result = 'Cluster with centroid '\
               + str(self.centroid.getFeatures()) + ' contains:\n  '
        for e in names:
            result = result + e + ', '
        return result[:-2] #remove trailing comma and space    
```

Dissimilarity function:

```python
def dissimilarity(clusters):
    """Assumes clusters a list of clusters
       Returns a measure of the total dissimilarity of the
       clusters in the list"""
    totDist = 0
    for c in clusters:
        totDist += c.variability()
    return totDist
```

Patients in lect12.py:

```python
import cluster
import random, pylab, numpy

class Patient(cluster.Example):
    pass

def scaleAttrs(vals): # Z-Scaling
    vals = pylab.array(vals)
    mean = sum(vals)/len(vals)
    sd = numpy.std(vals)
    vals = vals - mean
    return vals/sd

def getData(toScale = False):
    #read in data
    hrList, stElevList, ageList, prevACSList, classList = [],[],[],[],[]
    cardiacData = open('cardiacData.txt', 'r')
    for l in cardiacData:
        l = l.split(',')
        hrList.append(int(l[0]))
        stElevList.append(int(l[1]))
        ageList.append(int(l[2]))
        prevACSList.append(int(l[3]))
        classList.append(int(l[4]))
    if toScale:
        hrList = scaleAttrs(hrList)
        stElevList = scaleAttrs(stElevList)
        ageList = scaleAttrs(ageList)
        prevACSList = scaleAttrs(prevACSList)
    #Build points
    points = []
    for i in range(len(hrList)):
        features = pylab.array([hrList[i], prevACSList[i],\
                                stElevList[i], ageList[i]])
        pIndex = str(i)
        points.append(Patient('P'+ pIndex, features, classList[i]))
    return points
```

```python
def kmeans(examples, k, verbose = False):
    #Get k randomly chosen initial centroids, create cluster for each
    initialCentroids = random.sample(examples, k)
    clusters = []
    for e in initialCentroids:
        clusters.append(cluster.Cluster([e]))
        
    #Iterate until centroids do not change
    converged = False
    numIterations = 0
    while not converged:
        numIterations += 1
        #Create a list containing k distinct empty lists
        newClusters = []
        for i in range(k):
            newClusters.append([])
            
        #Associate each example with closest centroid
        for e in examples:
            #Find the centroid closest to e
            smallestDistance = e.distance(clusters[0].getCentroid())
            index = 0
            for i in range(1, k):
                distance = e.distance(clusters[i].getCentroid())
                if distance < smallestDistance:
                    smallestDistance = distance
                    index = i
            #Add e to the list of examples for appropriate cluster
            newClusters[index].append(e)
            
        for c in newClusters: #Avoid having empty clusters
            if len(c) == 0:
                raise ValueError('Empty Cluster')
        
        #Update each cluster; check if a centroid has changed
        converged = True
        for i in range(k):
            if clusters[i].update(newClusters[i]) > 0.0:
                converged = False
        if verbose:
            print('Iteration #' + str(numIterations))
            for c in clusters:
                print(c)
            print('') #add blank line
    return clusters
```

```python
def trykmeans(examples, numClusters, numTrials, verbose = False):
    """Calls kmeans numTrials times and returns the result with the
          lowest dissimilarity"""
    best = kmeans(examples, numClusters, verbose)
    minDissimilarity = cluster.dissimilarity(best)
    trial = 1
    while trial < numTrials:
        try:
            clusters = kmeans(examples, numClusters, verbose)
        except ValueError:
            continue #If failed, try again
        currDissimilarity = cluster.dissimilarity(clusters)
        if currDissimilarity < minDissimilarity:
            best = clusters
            minDissimilarity = currDissimilarity
        trial += 1
    return best

def printClustering(clustering):
    """Assumes: clustering is a sequence of clusters
       Prints information about each cluster
       Returns list of fraction of pos cases in each cluster"""
    posFracs = []
    for c in clustering:
        numPts = 0
        numPos = 0
        for p in c.members():
            numPts += 1
            if p.getLabel() == 1:
                numPos += 1
        fracPos = numPos/numPts
        posFracs.append(fracPos)
        print('Cluster of size', numPts, 'with fraction of positives =',
              round(fracPos, 4))
    return pylab.array(posFracs)

def testClustering(patients, numClusters, seed = 0, numTrials = 5):
    random.seed(seed)
    bestClustering = trykmeans(patients, numClusters, numTrials)
    posFracs = printClustering(bestClustering)
    return posFracs

patients = getData()
for k in (2,):
    print('\n     Test k-means (k = ' + str(k) + ')')
    posFracs = testClustering(patients, k, 2)

#numPos = 0
#for p in patients:
#    if p.getLabel() == 1:
#        numPos += 1
#print('Total number of positive patients =', numPos)
```

Different subgroups of positive patients have different characteristics.

How might we test this?

Try some other values of k.

```python
patients = getData()
for k in (2,4,6):
    print('\n     Test k-means (k = ' + str(k) + ')')
    posFracs = testClustering(patients, k, 2)
```

