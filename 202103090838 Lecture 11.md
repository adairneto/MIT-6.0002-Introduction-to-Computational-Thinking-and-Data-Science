# Lecture 11: Introduction to Machine Learning

Tags: #algorithms #python #machinelearning

Classification Methods: label data; k nearest neighbor

Clustering Methods: don't have label data; k-means

## What is Machine Learning?

> Field of study that gives computers the ability to learn without being explicitly programmed.
>
> Arthur Samuel (1959)

Traditional programming: give the computer data and program. The computer returns the output.

Machine Learning: give the computer the data and the output. The computer will return the program.

## How are things learned?

- Memorization (Declarative Knowledge)
  - Accumulation of facts
  - Limited by time and memory
- Generalization (Imperative Knowledge)
  - Deduce new facts from old facts
  - Limited by accuracy of deduction process

Programs that can infer useful information from implicit patterns in data.

**Basic Paradigm:**

- Observe set of examples: training set
- Infer something about process that generated that data
- Use inference to make prediction about previously unseen data: test set

## Supervised learning and unsupervised learning

- Supervised learning
  - Given a set of feature/label pairs, find a rule that predicts the label associated with a previously unseen input
- Unsupervised learning
  - Given a set of feature vectores (without labels) group them into "natural clusters" (or create labels for groups)
  - What is the best grouping that I can find?

## Clustering

Want to decide on "similarity" of examples with the goal of separating into distinct, "natural", groups.

Similarity is a **distance measure**. We want groups that are close to each other but far from the other group.

If there are k different groups in the training data,

- Pick k samples as examplars
- Cluster remaining samples by minimizing the distance between samples in the same cluster (objective function), put sample in the group with closest exemplar
- Find median example in each cluster as new exemplar
- Repeat until no change

## Classifying

If the data is labeled,

- We want to find the subsurface that separates the groups
- In the 2D space, find the line that best separates two groups
- When examples overlap, may have to tradeoff false positives/negatives

Both clustering and classifying can assign label to new data.

## Feature engineering

- **Requirements for ML:**
  - Choosing training data and evaluation method
  - Representation of the features
  - Distance metric for feature vectors
  - Objective function and constraints
  - Optimization method for learning the model
- Features never fully describe the situation ("all models are wrong, but some are useful")
- Represent examples by feature vectors that will facilitate generalization
- **Signal-to-Noise Ratio:** How to maximize ratio of useful input to irrelevant input?
- How to weight relative importance of different dimensions of feature vector?

## Minkowski metric: Manhattan distance and Euclidean distance

$$
dist(X1,X2,p) = \left( \sum_{k=1}^{len} |{X1}_k - {X2}_k|^{p} \right)^{\frac{1}{p}}
$$

p = 1: Manhattan Distance

p = 2: Euclidean Distance

```python
def minkowskiDist(v1, v2, p):
    """Assumes v1 and v2 are equal-length arrays of numbers
       Returns Minkowski distance of order p between v1 and v2"""
    dist = 0.0
    for i in range(len(v1)):
        dist += abs(v1[i] - v2[i])**p
    return dist**(1.0/p)
```

If one dimension is particularly large, one possible solution is to use binary features.

**Issues of Concern:**

- Distance metric between examples
- Choice of feature vectors
- Constraints on complexity of model

## Classification example

- Find boundaries in feature space that separate different classes of labeled examples
  - Look for simple surface (line or plane) that separates classes
  - Look for more complex surfaces (subject to constraints) that separates classes
  - Use voting schemes: find k nearest training examples, use majority vote to select label
- Issues:
  - How to avoid over-fitting?
  - How to measure performance?
  - How to select best features?
- How to do it:
  - Randomly divide data into training set and test set (50/50)
  - Attempt to minimize error on training data

### Confusion matrix and accuracy

- Look the diagonal (the corrected labeled results)

$$
\text{accuracy} = \frac{\text{true positive}+\text{true negative}}{\text{true positive} + \text{true negative} + \text{false positive} + \text{false negative}}
$$

- If both models give the same accuracy, which one is better?

### Other measurements: positive predictive value, sensitivity and specificity 

- PPV - Positive Predicted Value:

$$
\text{positive predicted value} = \frac{\text{true positive}}{\text{true positive} + \text{false positive}}
$$

- Sensitivity (percentage correctly found)

$$
\text{sensitivity} = \frac{\text{true positive}}{\text{true positive} + \text{false negative}}
$$

- Specificity (percentage correctly rejected)

$$
\text{specificity} = \frac{\text{true negative}}{\text{true negative} + \text{false positive}}
$$