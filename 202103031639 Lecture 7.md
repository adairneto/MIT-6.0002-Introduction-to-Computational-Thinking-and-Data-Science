# Lecture 7: Confidence Intervals

Tags: #algorithms #python #modeling #simulation #montecarlo

```python
import random, pylab, numpy

#set line width
pylab.rcParams['lines.linewidth'] = 4
#set font size for titles 
pylab.rcParams['axes.titlesize'] = 20
#set font size for labels on axes
pylab.rcParams['axes.labelsize'] = 20
#set size of numbers on x-axis
pylab.rcParams['xtick.labelsize'] = 16
#set size of numbers on y-axis
pylab.rcParams['ytick.labelsize'] = 16
#set size of ticks on x-axis
pylab.rcParams['xtick.major.size'] = 7
#set size of ticks on y-axis
pylab.rcParams['ytick.major.size'] = 7
#set size of markers, e.g., circles representing points
#set numpoints for legend
pylab.rcParams['legend.numpoints'] = 1
```

Mean estimation error is zero.

The distribution of errors is normal.

```python
random.seed(1)
dist, numSamples = [], 1000000

for i in range(numSamples):
   dist.append(random.gauss(0, 100))
   
weights = [1/numSamples]*len(dist)
v = pylab.hist(dist, bins = 100,
              weights = [1/numSamples]*len(dist))
              
print('Fraction within ~200 of mean =',
     sum(v[0][30:70]))
```

```python
def gaussian(x, mu, sigma):
  factor1 = (1.0/(sigma*((2*pylab.pi)**0.5)))
  factor2 = pylab.e**-(((x-mu)**2)/(2*sigma**2))
  return factor1*factor2
  
xVals, yVals = [], []
mu, sigma = 0, 1
x = -4
while x <= 4:
    xVals.append(x)
    yVals.append(gaussian(x, mu, sigma))
    x += 0.05
pylab.plot(xVals, yVals)
pylab.title('Normal Distribution, mu = ' + str(mu)\
            + ', sigma = ' + str(sigma))
```

Are values on y-axis probabilities? No, the probability of each point is different.

What are they? They're densities, i.e. a derivative of cumulative distribution function.

Once we have the area, then we can talk about the probabilities.

## SciPy

- Useful mathematical functions
- `scipy.integrate.quad` takes four arguments: function to be integrated; lower limit; upper limit; tuple supplying all values for all arguments, except the first. Returns a tuple of an approximation and the error.

```python
import scipy.integrate

def checkEmpirical(numTrials):
  for t in range(numTrials):
     mu = random.randint(-10, 10)
     sigma = random.randint(1, 10)
     print('For mu =', mu, 'and sigma =', sigma)
     for numStd in (1, 1.96, 3):
        area = scipy.integrate.quad(gaussian,
                                    mu-numStd*sigma,
                                    mu+numStd*sigma,
                                    (mu, sigma))[0]
        print(' Fraction within', numStd,
              'std =', round(area, 4))
```

The empirical rule actually works!

## Why use Normal Distributions?

- They occur a lot.
- Nice mathematical properties.

But not everything is normal. E.g. roulette. The outcomes are uniformly distributed. Each outcome is equally probable.

So why the empirical rule works here? 

We are working with the mean of a set of spins. The Central Limit Theorem applies.

### Central Limit Theorem

Given a sufficiently large sample:

1. The means of the samples in a set of samples (the sample means) will be approximately normally distributed.
2. This normal distribution will have a mean close to the mean of the population.
3. The variance of the sample means will be close to the variance of the population divided by the sample size.

```python
def plotMeans(numDice, numRolls, numBins, legend, color, style):
    means = []
    for i in range(numRolls//numDice):
        vals = 0
        for j in range(numDice):
            vals += 5*random.random() 
        means.append(vals/float(numDice))
    pylab.hist(means, numBins, color = color, label = legend,
              weights = [1/len(means)]*len(means),
              hatch = style)
    return getMeanAndStd(means)

mean, std = plotMeans(1, 1000000, 19, '1 die', 'b', '*')
print('Mean of rolling 1 die =', str(mean) + ',', 'Std =', std)
mean, std = plotMeans(50, 1000000, 19, 'Mean of 50 dice', 'r', '//')
print('Mean of rolling 50 dice =', str(mean) + ',', 'Std =', std)
pylab.title('Rolling Continuous Dice')
pylab.xlabel('Value')
pylab.ylabel('Probability')
pylab.legend()
```

Trying for roulette:

```python
numTrials = 1000000
numSpins = 200
game = FairRoulette()

means = []
for i in range(numTrials):
    means.append(findPocketReturn(game, 1, numSpins,
                                  False)[0])

pylab.hist(means, bins = 19,
          weights = [1/len(means)]*len(means))
pylab.xlabel('Mean Return')
pylab.ylabel('Probability')
pylab.title('Expected Return Betting a Pocket 200 Times')
```

The CLT allows us to use the empirical rule when computing confidence intervals.

## Buffon-Laplace Method for Pi

```python
def throwNeedles(numNeedles):
    inCircle = 0
    for Needles in range(1, numNeedles + 1, 1):
        x = random.random()
        y = random.random()
        if (x*x + y*y)**0.5 <= 1.0:
            inCircle += 1
    return 4*(inCircle/float(numNeedles))
    
def getEst(numNeedles, numTrials):
    estimates = []
    for t in range(numTrials):
        piGuess = throwNeedles(numNeedles)
        estimates.append(piGuess)
    sDev = numpy.std(estimates)
    curEst = sum(estimates)/len(estimates)
    print('Est. = ' + str(curEst) +\
          ', Std. dev. = ' + str(round(sDev, 6))\
          + ', Needles = ' + str(numNeedles))
    return (curEst, sDev)

def estPi(precision, numTrials):
    numNeedles = 1000
    sDev = precision
    while sDev >= precision/2:
        curEst, sDev = getEst(numNeedles,
                              numTrials)
        numNeedles *= 2
    return curEst

random.seed(0)
estPi(0.005, 100)
```

Are the estimates getting better? Not always, but the trend is good.

Does the standard deviation guarantees that we're getting closer?

Statisticaly valid is not the same as true. Example: introduce a bug by changing "4" to "2" on `throwNeedles`.

Strategy to compute and area: take an enclosing region that is easy to calculate; take some random points and inside the larger area; multiply the larger area by the fraction of points that fall within the smaller area.

How to use randomness to compute something that is not random.