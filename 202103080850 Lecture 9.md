# Lecture 9: Understanding Experimental Data

Tags: #algorithms #python #modeling #simulation #datascience #rsquared

## Introduction

How to manipulate data?

What does the software tells?

Statistics + Experimental Science.

Given a set of data, what can be done with it?

How to design a computation to complement the gathered data.

Computation as additional experiments.

Initial code:

```python
import random, pylab, numpy

#set line width
pylab.rcParams['lines.linewidth'] = 4
#set font size for titles 
pylab.rcParams['axes.titlesize'] = 20
#set font size for labels on axes
pylab.rcParams['axes.labelsize'] = 20
#set size of numbers on x-axis
pylab.rcParams['xtick.labelsize'] = 16
#set size of numbers on y-axis
pylab.rcParams['ytick.labelsize'] = 16
#set size of ticks on x-axis
pylab.rcParams['xtick.major.size'] = 7
#set size of ticks on y-axis
pylab.rcParams['ytick.major.size'] = 7
#set size of markers
pylab.rcParams['lines.markersize'] = 10
#set number of examples shown in legends
pylab.rcParams['legend.numpoints'] = 1

def getData(fileName):
    dataFile = open(fileName, 'r')
    distances = []
    masses = []
    dataFile.readline() #discard header
    for line in dataFile:
        d, m = line.split()
        distances.append(float(d))
        masses.append(float(m))
    dataFile.close()
    return (masses, distances)
    
def labelPlot():
    pylab.title('Measured Displacement of Spring')
    pylab.xlabel('|Force| (Newtons)')
    pylab.ylabel('Distance (meters)')
```

## Example 1: Modeling a Spring

Linear spring.

Spring constant $k$.

Hooke's Law: $F = -kd$.

It doesn't hold perfectly nor for all springs. There's a elastic limit of the spring.

**How to get the spring constant?** Put a block of mass $m$ at the bottom of the spring and measure the distance.

We shouldn't use just one data but a set of data.

**Plotting the data:**

```python
def plotData(fileName):
    xVals, yVals = getData(fileName)
    xVals = pylab.array(xVals)
    yVals = pylab.array(yVals)
    xVals = xVals*9.81  #acc. due to gravity
    pylab.plot(xVals, yVals, 'bo',
               label = 'Measured displacements')
    labelPlot()
```

**How do we fit a curve to data?**

1. Define an **objective function**.
2. Find the curve that minimizes it.

We'll measure the distance from the point $p$ to the curve by picking y-value, which is what we want to predict for given independent x-value. The y measures the error in that prediction.

**Least Squares Objective Function:**
$$
\sum_{i=0}^{len(observed)-1}(observed[i]-predicted[i])^2
$$
It's variance times the number of observations.

Tells us how badly is the prediction. Minimizing it, we'll minimize the variance.

To minimize this objective function, use **linear regression** to find a polynomial representation for the predicted model.

Linear regression is leaving at the highest point and going straight to the minimum, measuring the gradient.

### polyFit

`pylab.polyfit(observedX, observedY, n)`

```python
def fitData(fileName):
    xVals, yVals = getData(fileName)
    xVals = pylab.array(xVals)
    yVals = pylab.array(yVals)
    xVals = xVals*9.81 #get force
    pylab.plot(xVals, yVals, 'bo',
               label = 'Measured points')
    labelPlot()                 
    a,b = pylab.polyfit(xVals, yVals, 1)
    estYVals = a*pylab.array(xVals) + b
    print('a =', a, 'b =', b)
    pylab.plot(xVals, estYVals, 'r',
               label = 'Linear fit, k = '
               + str(round(1/a, 5)))
    pylab.legend(loc = 'best')
```

Version using **polyval**:

```python
def fitData1(fileName):
    xVals, yVals = getData(fileName)
    xVals = pylab.array(xVals)
    yVals = pylab.array(yVals)
    xVals = xVals*9.81 #get force
    pylab.plot(xVals, yVals, 'bo',
               label = 'Measured points')
    labelPlot()                 
    model = pylab.polyfit(xVals, yVals, 1)
    estYVals = pylab.polyval(model, xVals)
    pylab.plot(xVals, estYVals, 'r',
               label = 'Linear fit, k = '
               + str(round(1/model[0], 5)))
    pylab.legend(loc = 'best')
```

Note that we can use different orders with polyval.

## Example 2: Using Mystery Data

The line isn't enough. We're going to do a Linear Regression fitting the best parabola to the data.

```python
xVals, yVals = getData('mysteryData.txt')
pylab.plot(xVals, yVals, 'o', label = 'Data Points')
pylab.title('Mystery Data')

#Try linear model
model1 = pylab.polyfit(xVals, yVals, 1)
pylab.plot(xVals, pylab.polyval(model1, xVals),
          label = 'Linear Model')

#Try a quadratic model
model2 = pylab.polyfit(xVals, yVals, 2)
pylab.plot(xVals, pylab.polyval(model2, xVals),
          'r--', label = 'Quadratic Model')
pylab.legend()
```

How to decide which one is better?

## How Good are these Fits?

### Relative to each other

```python
def aveMeanSquareError(data, predicted):
    error = 0.0
    for i in range(len(data)):
        error += (data[i] - predicted[i])**2
    return error/len(data)

# code to compare fits for mystery data
estYVals = pylab.polyval(model1, xVals)  
print('Ave. mean square error for linear model =',
      aveMeanSquareError(yVals, estYVals))
estYVals = pylab.polyval(model2, xVals)
print('Ave. mean square error for quadratic model =',
      aveMeanSquareError(yVals, estYVals))
```

Used to compare two different models for the same data. But not good to get a sense of absolute goodness of fit.

### In an Absolute Sense (R-squared)

$$
R^2 = 1 - \frac{\sum_{i}(y_i - p_i)^2}{\sum_{i}(y_i - \mu)^2}
$$

- Where $y_i$ are the measured values.
- $p_i$ are the predicted values.
- $\mu$ is the mean of measured values.
- Numerator: sum of squared errors.
- Denominator is variance times number of samples.

```python
def rSquared(observed, predicted):
    error = ((predicted - observed)**2).sum()
    meanError = error/len(observed)
    return 1 - (meanError/numpy.var(observed))
```

Clever trick: mean SSE/variance is same as $R^2$ ratio.

$R^2=0$ the model doesn't capture anything.

$R^2=1$ means that the model captures all the variability in the data.

```python
def genFits(xVals, yVals, degrees):
    models = []
    for d in degrees:
        model = pylab.polyfit(xVals, yVals, d)
        models.append(model)
    return models

def testFits(models, degrees, xVals, yVals, title):
    pylab.plot(xVals, yVals, 'o', label = 'Data')
    for i in range(len(models)):
        estYVals = pylab.polyval(models[i], xVals)
        error = rSquared(yVals, estYVals)
        pylab.plot(xVals, estYVals,
                   label = 'Fit of degree '\
                   + str(degrees[i])\
                   + ', R2 = ' + str(round(error, 5)))
    pylab.legend(loc = 'best')
    pylab.title(title)
```

What about even bigger orders?

Looking better/closer means that we should use it?